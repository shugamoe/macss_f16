---
title: "STAT 374 | HW1 "
author: "Julian McClellan"
date: "April 23, 2016"
output:
  pdf_document:
    toc: true
    highlight: zenburn
    fig_height: 3.5
---
## 1. Computing and plotting with R(15 points)

### (a)



```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
sample_size = c(seq(1,200))
sim = function(part_b = FALSE, ssize = sample_size, sig = 1, b = 100) {
  rv = c()
  for(n in sample_size){
    stat_n =  c()
    
    for(rep in (1:b)) {
      mew_n = mean(rnorm(n, 1, sig))
      if(part_b) {
        stat_b = sqrt(n) * (mew_n - 1) # Z
      } else {
        stat_b = (mew_n - 1)^2 # The MSE
      }
      stat_n = append(stat_n, stat_b)
    }
    
    if(part_b) {
      stat_rv = stat_n
    } else {
      stat_rv = mean(stat_n)
    }
    rv = append(rv, stat_rv) 
  }
  if(part_b) {
    x = seq(-4, 4, .01)
    f = dnorm(x, sd = sig)
    plot(density(rv), main = "Z")
    lines(x,f,'l',col='red')
  } else {
    # Normal scale plot
    plot(sample_size, rv, ylab = "")
    lines(sample_size, (sig^2) / sample_size, col = "red")
    title(expression(paste(frac(sigma^2, n), "vs. Empirical MSE")), line = -2) 
    
    # Log Log Plot
    plot(sample_size, log = "xy", rv, ylab = "log 1", xlab = "log sample size")
    lines(sample_size, (sig^2) / sample_size, col = "red")
    title(expression(paste("log(", frac(sigma^2, n), ") ", "vs. log(Empirical MSE)")), line = -2)
  }
  
}
```

### (b)
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
zs = sim(TRUE)
plot(sample_size, zs)
```

## 2. Leave-one-out cross-validation (20 points)

### (a)

### (b)

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60), echo=FALSE}
# tricube = function(x) {
#   return(70 / 81 * (1 - abs(x) ^ 3) ^ 3 * ind(x))
# }
# 
# ind = function(x) {
#   if(abs(x) <= 1) {
#     return(1)
#   } else {
#     return(0)
#   }
# }
# 
# S_nj = function(obs, j, x, h = .01) {
#   n = length(obs)
#   return(sum(tricube((obs - x) / h) * (obs - x) ^ j))
# }
# 
# b = function(obs, x, h = .01) {
#   return(tricube((obs - x) / h) * (S_nj(obs, 2, x) - (obs - x) * S_nj(obs, 1, x)))
# }
# 
# l = function(obs, x, h = .01) {
#   return(b(obs, x) / sum(b(obs, x)))
# }
# 
# lsmoother = function (data, x, h = .01) {
#   # Utilizing Theorem 5.60 from Textbook
#   obs = data[,1]
#   y = data[,2]
# 
#   return(sum(l(obs, x) * y))
# }
```

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=80), fig.show='hold'}
doppler = function(x) {
  return(sqrt(x * (1 - x))*sin(2.1 * pi/(x + .05)))
}

cvs = function(h, x, y) {
  llr = locfit(y ~ x, alpha = c(0, h), deg = 1)
  
  # From 'smoothdemo.r'
  Lii = predict(llr, where="data", what="infl")
  
  return(mean(((y - fitted(llr)) / (1 - Lii)) ^ 2))
}

model = function(num_obs = 1000, sigma = 0.1) {
  # Generate data
  x = (1:num_obs) / num_obs
  y = doppler(x) + sigma * rnorm(num_obs)
  
  # Determine optimal h (using the function cvs as the "formula derived in part (a)")
  h = seq(.01, .8, .01)
  cv_scores = unlist(lapply(h, cvs, x = x, y = y))
  hstar = h[cv_scores == min(cv_scores)]
  
  # Local linear regression fitted using locfit lib
  out = locfit(y ~ x, alpha = c(0, hstar), deg = 1)
  plot(x, y, pch=16, cex=0.9, col=rgb(0.7, 0.7, 0.9))
  lines(x, doppler(x), 'l', col='red', lwd=3)
  lines(x, fitted(out), 'l', col='blue', lwd=3)
  
  # From 'smoothdemo.r'
  normell = predict(out, where='data', what='vari')
  n = length(x)
  lines(x, fitted(out) + sqrt(n) * 2 * sigma * normell, 'l', col='gray', lwd=1)
  lines(x, fitted(out) - sqrt(n) * 2 * sigma * normell, 'l', col='gray', lwd=1)
  
}
# Run model
library(locfit)
model()
```

Is $I_{n}(x)$ the 95 percent pointwise confidence interval for $r(x)$?

## 3. Kernel density estimates for Old Faithful (15 points)

```{r, tidy.opts=list(width.cutoff=80), fig.show='hold'}
data(faithful)

```

