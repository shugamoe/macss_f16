i = 2
x[x != x[i]]
test = function() {
return(PENIS)
}
model = function(num_obs = 1000, sig = 0.1) {
xs = (1:num_obs) / num_obs
ys = doppler(xs) + sig * rnorm(num_obs)
PENIS = 5
test()
}
test = function() {
return(PENIS)
}
model = function(num_obs = 1000, sig = 0.1) {
xs = (1:num_obs) / num_obs
ys = doppler(xs) + sig * rnorm(num_obs)
PENIS = 5
return(test())
}
model()
x
y
y = append(y, c(5,7,8))
5+=5
5 += 5
x
x+=5
x += 5
x
length(x
)
sample_size = c(seq(1,200))
sim = function(part_b = FALSE, ssize = sample_size, sig = 1, b = 100) {
rv = c()
for(n in sample_size){
stat_n =  c()
for(rep in (1:b)) {
mew_n = mean(rnorm(n, 1, sig))
if(part_b) {
stat_b = sqrt(n) * (mew_n - 1) # Z
} else {
stat_b = (mew_n - 1)^2 # The MSE
}
stat_n = append(stat_n, stat_b)
}
if(part_b) {
stat_rv = stat_n
} else {
stat_rv = mean(stat_n)
}
rv = append(rv, stat_rv)
}
if(part_b) {
x = seq(-4, 4, .01)
f = dnorm(x, sd = sig)
plot(density(rv), main = "Z")
lines(x,f,'l',col='red')
} else {
# Normal scale plot
plot(sample_size, rv, ylab = "")
lines(sample_size, (sig^2) / sample_size, col = "red")
title(expression(paste(frac(sigma^2, n), "vs. Empirical MSE")), line = -2)
# Log Log Plot
plot(sample_size, log = "xy", rv, ylab = "log 1", xlab = "log sample size")
lines(sample_size, (sig^2) / sample_size, col = "red")
title(expression(paste("log(", frac(sigma^2, n), ") ", "vs. log(Empirical MSE)")), line = -2)
}
}
sim()
doppler = function(x) {
return(sqrt(x * (1 - x))*sin(2.1 * pi/(x + .05)))
}
# From 2a
# [Q] This function still seems to indicate that the smallest possible h given is the optimal h.
cvs = function(h, x, y) {
llr = locfit(y ~ x, alpha = c(0, h), deg = 1, maxk = 100000)
# From 'smoothdemo.r'
Lii = predict(llr, where="data", what="infl")
return(mean(((y - fitted(llr)) / (1 - Lii)) ^ 2))
}
model = function(num_obs = 1000, sigma = 0.1) {
# Generate data
x = (1:num_obs) / num_obs
y = doppler(x) + sigma * rnorm(num_obs)
# Determine optimal h
h = seq(.01, .8, .01)
alphas = cbind(rep(0, length(h)), h)
gcvs = gcvplot(y ~ x, alpha=alphas, deg = 1)
cv_scores = gcvs$values
plot(h, cv_scores, type = 'l' main = 'Cross validation scores versus bandwidth (h)')
hstar = h[cv_scores == min(cv_scores)]
print(hstar)
# Local linear regression fitted using locfit lib
out = locfit(y ~ x, alpha = c(0, hstar), deg = 1)
plot(x, y, pch=16, cex=0.9, col=rgb(0.7, 0.7, 0.9))
lines(x, doppler(x), 'l', col='red', lwd=3)
lines(x, fitted(out), 'l', col='blue', lwd=3)
# From 'smoothdemo.r'
normell = predict(out, where='data', what='vari')
n = length(x)
lines(x, fitted(out) + sqrt(n) * 2 * sigma * normell, 'l', col='gray', lwd=1)
lines(x, fitted(out) - sqrt(n) * 2 * sigma * normell, 'l', col='gray', lwd=1)
title("Plot of data, local linear estimates, and the Confidence Interval")
}
# Run model
library(locfit)
model()
doppler = function(x) {
return(sqrt(x * (1 - x))*sin(2.1 * pi/(x + .05)))
}
# From 2a
# [Q] This function still seems to indicate that the smallest possible h given is the optimal h.
cvs = function(h, x, y) {
llr = locfit(y ~ x, alpha = c(0, h), deg = 1, maxk = 100000)
# From 'smoothdemo.r'
Lii = predict(llr, where="data", what="infl")
return(mean(((y - fitted(llr)) / (1 - Lii)) ^ 2))
}
model = function(num_obs = 1000, sigma = 0.1) {
# Generate data
x = (1:num_obs) / num_obs
y = doppler(x) + sigma * rnorm(num_obs)
# Determine optimal h
h = seq(.01, .8, .01)
alphas = cbind(rep(0, length(h)), h)
gcvs = gcvplot(y ~ x, alpha=alphas, deg = 1)
cv_scores = gcvs$values
plot(h, cv_scores, type = 'l' main = 'Cross validation scores versus bandwidth (h)')
hstar = h[cv_scores == min(cv_scores)]
print(hstar)
# Local linear regression fitted using locfit lib
out = locfit(y ~ x, alpha = c(0, hstar), deg = 1)
plot(x, y, pch=16, cex=0.9, col=rgb(0.7, 0.7, 0.9))
lines(x, doppler(x), 'l', col='red', lwd=3)
lines(x, fitted(out), 'l', col='blue', lwd=3)
# From 'smoothdemo.r'
normell = predict(out, where='data', what='vari')
n = length(x)
lines(x, fitted(out) + sqrt(n) * 2 * sigma * normell, 'l', col='gray', lwd=1)
lines(x, fitted(out) - sqrt(n) * 2 * sigma * normell, 'l', col='gray', lwd=1)
title("Plot of data, local linear estimates, and the Confidence Interval")
}
# Run model
library(locfit)
model()
doppler = function(x) {
return(sqrt(x * (1 - x))*sin(2.1 * pi/(x + .05)))
}
# From 2a
# [Q] This function still seems to indicate that the smallest possible h given is the optimal h.
cvs = function(h, x, y) {
llr = locfit(y ~ x, alpha = c(0, h), deg = 1, maxk = 100000)
# From 'smoothdemo.r'
Lii = predict(llr, where="data", what="infl")
return(mean(((y - fitted(llr)) / (1 - Lii)) ^ 2))
}
model = function(num_obs = 1000, sigma = 0.1) {
# Generate data
x = (1:num_obs) / num_obs
y = doppler(x) + sigma * rnorm(num_obs)
# Determine optimal h
h = seq(.01, .8, .01)
alphas = cbind(rep(0, length(h)), h)
gcvs = gcvplot(y ~ x, alpha=alphas, deg = 1)
cv_scores = gcvs$values
plot(h, cv_scores, type = 'l' main = 'Cross validation scores versus bandwidth (h)')
hstar = h[cv_scores == min(cv_scores)]
print(hstar)
# Local linear regression fitted using locfit lib
out = locfit(y ~ x, alpha = c(0, hstar), deg = 1)
plot(x, y, pch=16, cex=0.9, col=rgb(0.7, 0.7, 0.9))
lines(x, doppler(x), 'l', col='red', lwd=3)
lines(x, fitted(out), 'l', col='blue', lwd=3)
# From 'smoothdemo.r'
normell = predict(out, where='data', what='vari')
n = length(x)
lines(x, fitted(out) + sqrt(n) * 2 * sigma * normell, 'l', col='gray', lwd=1)
lines(x, fitted(out) - sqrt(n) * 2 * sigma * normell, 'l', col='gray', lwd=1)
title("Plot of data, local linear estimates, and the Confidence Interval")
}
# Run model
library(locfit)
model()
doppler = function(x) {
return(sqrt(x * (1 - x))*sin(2.1 * pi/(x + .05)))
}
# From 2a
# [Q] This function still seems to indicate that the smallest possible h given is the optimal h.
cvs = function(h, x, y) {
llr = locfit(y ~ x, alpha = c(0, h), deg = 1, maxk = 100000)
# From 'smoothdemo.r'
Lii = predict(llr, where="data", what="infl")
return(mean(((y - fitted(llr)) / (1 - Lii)) ^ 2))
}
model = function(num_obs = 1000, sigma = 0.1) {
# Generate data
x = (1:num_obs) / num_obs
y = doppler(x) + sigma * rnorm(num_obs)
# Determine optimal h
h = seq(.01, .8, .01)
alphas = cbind(rep(0, length(h)), h)
gcvs = gcvplot(y ~ x, alpha=alphas, deg = 1)
cv_scores = gcvs$values
plot(h, cv_scores, type = 'l', main = 'Cross validation scores versus bandwidth (h)')
hstar = h[cv_scores == min(cv_scores)]
print(hstar)
# Local linear regression fitted using locfit lib
out = locfit(y ~ x, alpha = c(0, hstar), deg = 1)
plot(x, y, pch=16, cex=0.9, col=rgb(0.7, 0.7, 0.9))
lines(x, doppler(x), 'l', col='red', lwd=3)
lines(x, fitted(out), 'l', col='blue', lwd=3)
# From 'smoothdemo.r'
normell = predict(out, where='data', what='vari')
n = length(x)
lines(x, fitted(out) + sqrt(n) * 2 * sigma * normell, 'l', col='gray', lwd=1)
lines(x, fitted(out) - sqrt(n) * 2 * sigma * normell, 'l', col='gray', lwd=1)
title("Plot of data, local linear estimates, and the Confidence Interval")
}
# Run model
library(locfit)
model()
doppler = function(x) {
return(sqrt(x * (1 - x))*sin(2.1 * pi/(x + .05)))
}
# From 2a
# [Q] This function still seems to indicate that the smallest possible h given is the optimal h.
cvs = function(h, x, y) {
llr = locfit(y ~ x, alpha = c(0, h), deg = 1, maxk = 100000)
# From 'smoothdemo.r'
Lii = predict(llr, where="data", what="infl")
return(mean(((y - fitted(llr)) / (1 - Lii)) ^ 2))
}
model = function(num_obs = 1000, sigma = 0.1) {
# Generate data
x = (1:num_obs) / num_obs
y = doppler(x) + sigma * rnorm(num_obs)
# Determine optimal h
h = seq(.001, .4, .001)
alphas = cbind(rep(0, length(h)), h)
gcvs = gcvplot(y ~ x, alpha=alphas, deg = 1, maxk=10000)
cv_scores = gcvs$values
plot(h, cv_scores, type = 'l', main = 'Cross validation scores versus bandwidth (h)')
hstar = h[cv_scores == min(cv_scores)]
print(hstar)
# Local linear regression fitted using locfit lib
out = locfit(y ~ x, alpha = c(0, hstar), deg = 1)
plot(x, y, pch=16, cex=0.9, col=rgb(0.7, 0.7, 0.9))
lines(x, doppler(x), 'l', col='red', lwd=3)
lines(x, fitted(out), 'l', col='blue', lwd=3)
# From 'smoothdemo.r'
normell = predict(out, where='data', what='vari')
n = length(x)
lines(x, fitted(out) + sqrt(n) * 2 * sigma * normell, 'l', col='gray', lwd=1)
lines(x, fitted(out) - sqrt(n) * 2 * sigma * normell, 'l', col='gray', lwd=1)
title("Plot of data, local linear estimates, and the Confidence Interval")
}
# Run model
library(locfit)
model()
doppler = function(x) {
return(sqrt(x * (1 - x))*sin(2.1 * pi/(x + .05)))
}
# From 2a
# [Q] This function still seems to indicate that the smallest possible h given is the optimal h.
cvs = function(h, x, y) {
llr = locfit(y ~ x, alpha = c(0, h), deg = 1, maxk = 1000000)
# From 'smoothdemo.r'
Lii = predict(llr, where="data", what="infl")
return(mean(((y - fitted(llr)) / (1 - Lii)) ^ 2))
}
model = function(num_obs = 1000, sigma = 0.1) {
# Generate data
x = (1:num_obs) / num_obs
y = doppler(x) + sigma * rnorm(num_obs)
# Determine optimal h
h = seq(.001, .4, .001)
alphas = cbind(rep(0, length(h)), h)
gcvs = gcvplot(y ~ x, alpha=alphas, deg = 1, maxk=10000)
cv_scores = gcvs$values
plot(h, cv_scores, type = 'l', main = 'Cross validation scores versus bandwidth (h)')
hstar = h[cv_scores == min(cv_scores)]
print(hstar)
# Local linear regression fitted using locfit lib
out = locfit(y ~ x, alpha = c(0, hstar), deg = 1)
plot(x, y, pch=16, cex=0.9, col=rgb(0.7, 0.7, 0.9))
lines(x, doppler(x), 'l', col='red', lwd=3)
lines(x, fitted(out), 'l', col='blue', lwd=3)
# From 'smoothdemo.r'
normell = predict(out, where='data', what='vari')
n = length(x)
lines(x, fitted(out) + sqrt(n) * 2 * sigma * normell, 'l', col='gray', lwd=1)
lines(x, fitted(out) - sqrt(n) * 2 * sigma * normell, 'l', col='gray', lwd=1)
title("Plot of data, local linear estimates, and the Confidence Interval")
}
# Run model
library(locfit)
model()
library(gam)
gam.model = gam(count ~ s(daylabel) + s(atemp) + s(humidity) + s(windspeed) + weather + hour + season + workingday + holiday + s(temp), data = new_train)
rmsle(new_test$count, predict(gam.model, new_test))
rmsle = function(trues, estimates) {
diff = trues - estimates
return(sqrt(mean(diff ^ 2)))
}
train = read.csv("train.csv", colClasses=c('year'="factor", "month"="factor", "hour"="factor", "season"="factor",
"holiday"="factor", "workingday"="factor", "weather"="factor"))
test = read.csv("test.csv", colClasses=c('year'="factor", "month"="factor", "hour"="factor", "season"="factor",
"holiday"="factor", "workingday"="factor", "weather"="factor"))
train$count = log(train$count + 1)
# Split the training data into new training and new test sets
new_train = train[which(train$day <= 15),]
new_test = train[which(16 <= train$day & train$day <= 19),]
# Change day to factor
new_train$day = as.factor(new_train$day)
new_test$day = as.factor(new_test$day)
test$day = as.factor(test$day)
lm0 = lm(count ~ workingday + holiday + season + daylabel + hour + weather + atemp + humidity*windspeed, data = new_train)
rmsle(new_test$count, predict(lm0, new_test))
library(gam)
gam.mcount_model = gam(mean_count ~ s(daylabel), data = new_train)
new_train$resids = resid(gam.mcount_model)
gam.resid_model = gam(resids ~ + s(atemp) + s(humidity) + s(windspeed) + weather + hour + season + workingday + holiday, data = new_train)
rmsle(new_test$count, predict(gam.resid_model, new_test) + predict(gam.mcount_model, new_test))
setwd("~/macss_f16/nonpar/hw1")
library(MASS)
library(locfit)
for(day_num in unique(new_train$daylabel)) {
mean_count = mean(new_train$count[which(new_train$daylabel == day_num)])
new_train$mean_count[which(new_train$daylabel == day_num)] = mean_count
}
plot(unique(new_train$daylabel), unique(new_train$mean_count), main = 'Means vs. Daylabel', ylab = 'Mean hourly log counts per day', xlab = 'Daylabel')
mcount.model = locfit(mean_count ~ daylabel, data = new_train)
new_train$resids = resid(mcount.model)
resid_model = lm(resids ~ workingday + holiday + season + daylabel + hour + weather + atemp + humidity*windspeed, data = new_train)
# Have to add the counts from the new_test set back back so that the predict() function actually predicts counts and not residuals.
# [Q] What do you mean by combine? My RMSLE is not that good now.
rmsle(new_test$count, predict(resid_model, new_test) + predict(mcount.model, new_test))
rmsle = function(trues, estimates) {
diff = trues - estimates
return(sqrt(mean(diff ^ 2)))
}
train = read.csv("train.csv", colClasses=c('year'="factor", "month"="factor", "hour"="factor", "season"="factor",
"holiday"="factor", "workingday"="factor", "weather"="factor"))
test = read.csv("test.csv", colClasses=c('year'="factor", "month"="factor", "hour"="factor", "season"="factor",
"holiday"="factor", "workingday"="factor", "weather"="factor"))
train$count = log(train$count + 1)
# Split the training data into new training and new test sets
new_train = train[which(train$day <= 15),]
new_test = train[which(16 <= train$day & train$day <= 19),]
# Change day to factor
new_train$day = as.factor(new_train$day)
new_test$day = as.factor(new_test$day)
test$day = as.factor(test$day)
lm0 = lm(count ~ workingday + holiday + season + daylabel + hour + weather + atemp + humidity*windspeed, data = new_train)
rmsle(new_test$count, predict(lm0, new_test))
library(MASS)
library(locfit)
for(day_num in unique(new_train$daylabel)) {
mean_count = mean(new_train$count[which(new_train$daylabel == day_num)])
new_train$mean_count[which(new_train$daylabel == day_num)] = mean_count
}
plot(unique(new_train$daylabel), unique(new_train$mean_count), main = 'Means vs. Daylabel', ylab = 'Mean hourly log counts per day', xlab = 'Daylabel')
mcount.model = locfit(mean_count ~ daylabel, data = new_train)
new_train$resids = resid(mcount.model)
resid_model = lm(resids ~ workingday + holiday + season + daylabel + hour + weather + atemp + humidity*windspeed, data = new_train)
# Have to add the counts from the new_test set back back so that the predict() function actually predicts counts and not residuals.
# [Q] What do you mean by combine? My RMSLE is not that good now.
rmsle(new_test$count, predict(resid_model, new_test) + predict(mcount.model, new_test))
library(gam)
gam.model = gam(count ~ s(daylabel) + s(atemp) + s(humidity) + s(windspeed) + weather + hour + season + workingday + holiday + s(temp), data = new_train)
rmsle(new_test$count, predict(gam.model, new_test))
library(gam)
gam.mcount_model = gam(mean_count ~ s(daylabel), data = new_train)
new_train$resids = resid(gam.mcount_model)
gam.resid_model = gam(resids ~ + s(atemp) + s(humidity) + s(windspeed) + weather + hour + season + workingday + holiday, data = new_train)
rmsle(new_test$count, predict(gam.resid_model, new_test) + predict(gam.mcount_model, new_test))
library(gam)
gam.mcount_model = gam(mean_count ~ s(daylabel), data = new_train)
new_train$resids = resid(gam.mcount_model)
gam.resid_model = gam(resids ~ + s(atemp) + s(humidity) + s(windspeed) + weather + hour + season + workingday + holiday, data = new_train)
rmsle(new_test$count, predict(gam.resid_model, new_test) + predict(gam.mcount_model, new_test))
head(new_train)
library(gam)
gam.mcount_model = gam(mean_count ~ s(daylabel), data = new_train)
new_train$resids = resid(gam.mcount_model)
gam.resid_model = gam(resids ~ + s(atemp) + s(humidity) + s(windspeed) + weather + hour + season + workingday + holiday, data = new_train)
predictions = predict(gam.mcount_model, test) + predict(gam.resid_model)
library(gam)
gam.mcount_model = gam(mean_count ~ s(daylabel), data = new_train)
new_train$resids = resid(gam.mcount_model)
gam.resid_model = gam(resids ~ + s(atemp) + s(humidity) + s(windspeed) + weather + hour + season + workingday + holiday, data = new_train)
predictions = predict(gam.mcount_model, test) + predict(gam.resid_model, test)
length(predictions)
library(gam)
gam.mcount_model = gam(mean_count ~ s(daylabel), data = new_train)
new_train$resids = resid(gam.mcount_model)
gam.resid_model = gam(resids ~ + s(atemp) + s(humidity) + s(windspeed) + weather + hour + season + workingday + holiday, data = new_train)
predictions = predict(gam.mcount_model, test) + predict(gam.resid_model, test)
write(predictions, file = 'assn1-jmcclellan.txt')
test = read.csv('assn1-jmcclellan.txt')
test
length(test)
library(gam)
gam.mcount_model = gam(mean_count ~ s(daylabel), data = new_train)
new_train$resids = resid(gam.mcount_model)
gam.resid_model = gam(resids ~ + s(atemp) + s(humidity) + s(windspeed) + weather + hour + season + workingday + holiday, data = new_train)
predictions = as.matrix(predict(gam.mcount_model, test) + predict(gam.resid_model, test))
write(predictions, file = 'assn1-jmcclellan.txt')
library(gam)
gam.mcount_model = gam(mean_count ~ s(daylabel), data = new_train)
new_train$resids = resid(gam.mcount_model)
gam.resid_model = gam(resids ~ + s(atemp) + s(humidity) + s(windspeed) + weather + hour + season + workingday + holiday, data = new_train)
predictions = predict(gam.mcount_model, test) + predict(gam.resid_model, test)
predictions = as.matrix(predictions)
write(predictions, file = 'assn1-jmcclellan.txt')
library(gam)
gam.mcount_model = gam(mean_count ~ s(daylabel), data = new_train)
new_train$resids = resid(gam.mcount_model)
gam.resid_model = gam(resids ~ + s(atemp) + s(humidity) + s(windspeed) + weather + hour + season + workingday + holiday, data = new_train)
predictions = predict(gam.mcount_model, test) + predict(gam.resid_model, test)
predictions = as.matrix(predictions)
write.csv(predictions, file = 'assn1-jmcclellan.txt')
library(gam)
gam.mcount_model = gam(mean_count ~ s(daylabel), data = new_train)
new_train$resids = resid(gam.mcount_model)
gam.resid_model = gam(resids ~ + s(atemp) + s(humidity) + s(windspeed) + weather + hour + season + workingday + holiday, data = new_train)
predictions = predict(gam.mcount_model, test) + predict(gam.resid_model, test)
predictions = e(predictions) - 1
write.csv(predictions, file = 'assn1-jmcclellan.txt')
library(gam)
gam.mcount_model = gam(mean_count ~ s(daylabel), data = new_train)
new_train$resids = resid(gam.mcount_model)
gam.resid_model = gam(resids ~ + s(atemp) + s(humidity) + s(windspeed) + weather + hour + season + workingday + holiday, data = new_train)
predictions = predict(gam.mcount_model, test) + predict(gam.resid_model, test)
predictions = exp(predictions) - 1
write.csv(predictions, file = 'assn1-jmcclellan.txt')
library(gam)
gam.mcount_model = gam(mean_count ~ s(daylabel), data = new_train)
new_train$resids = resid(gam.mcount_model)
gam.resid_model = gam(resids ~ + s(atemp) + s(humidity) + s(windspeed) + weather + hour + season + workingday + holiday, data = new_train)
predictions = predict(gam.mcount_model, test) + predict(gam.resid_model, test)
predictions = exp(predictions) - 1
write.csv(predictions, file = 'assn1-jmcclellan.txt')
head(test)
rmsle = function(trues, estimates) {
diff = trues - estimates
return(sqrt(mean(diff ^ 2)))
}
train = read.csv("train.csv", colClasses=c('year'="factor", "month"="factor", "hour"="factor", "season"="factor",
"holiday"="factor", "workingday"="factor", "weather"="factor"))
test = read.csv("test.csv", colClasses=c('year'="factor", "month"="factor", "hour"="factor", "season"="factor",
"holiday"="factor", "workingday"="factor", "weather"="factor"))
train$count = log(train$count + 1)
# Split the training data into new training and new test sets
new_train = train[which(train$day <= 15),]
new_test = train[which(16 <= train$day & train$day <= 19),]
# Change day to factor
new_train$day = as.factor(new_train$day)
new_test$day = as.factor(new_test$day)
test$day = as.factor(test$day)
lm0 = lm(count ~ workingday + holiday + season + daylabel + hour + weather + atemp + humidity*windspeed, data = new_train)
rmsle(new_test$count, predict(lm0, new_test))
library(MASS)
library(locfit)
for(day_num in unique(new_train$daylabel)) {
mean_count = mean(new_train$count[which(new_train$daylabel == day_num)])
new_train$mean_count[which(new_train$daylabel == day_num)] = mean_count
}
plot(unique(new_train$daylabel), unique(new_train$mean_count), main = 'Means vs. Daylabel', ylab = 'Mean hourly log counts per day', xlab = 'Daylabel')
mcount.model = locfit(mean_count ~ daylabel, data = new_train)
new_train$resids = resid(mcount.model)
resid_model = lm(resids ~ workingday + holiday + season + daylabel + hour + weather + atemp + humidity*windspeed, data = new_train)
rmsle(new_test$count, predict(resid_model, new_test) + predict(mcount.model, new_test))
library(gam)
gam.model = gam(count ~ s(daylabel) + s(atemp) + s(humidity) + s(windspeed) + weather + hour + season + workingday + holiday + s(temp), data = new_train)
rmsle(new_test$count, predict(gam.model, new_test))
library(gam)
gam.mcount_model = gam(mean_count ~ s(daylabel), data = new_train)
new_train$resids = resid(gam.mcount_model)
gam.resid_model = gam(resids ~ + s(atemp) + s(humidity) + s(windspeed) + weather + hour + season + workingday + holiday, data = new_train)
predictions = predict(gam.mcount_model, test) + predict(gam.resid_model, test)
predictions = exp(predictions) - 1
write.csv(predictions, file = 'assn1-jmcclellan.txt')
library(gam)
gam.mcount_model = gam(mean_count ~ s(daylabel), data = new_train)
new_train$resids = resid(gam.mcount_model)
gam.resid_model = gam(resids ~ + s(atemp) + s(humidity) + s(windspeed) + weather + hour + season + workingday + holiday, data = new_train)
predictions = predict(gam.mcount_model, test) + predict(gam.resid_model, test)
predictions = exp(predictions) - 1
write.csv(predictions, file = 'assn1-jmcclellan.txt', row.names = FALSE)
library(gam)
gam.mcount_model = gam(mean_count ~ s(daylabel), data = new_train)
new_train$resids = resid(gam.mcount_model)
gam.resid_model = gam(resids ~ + s(atemp) + s(humidity) + s(windspeed) + weather + hour + season + workingday + holiday, data = new_train)
predictions = predict(gam.model, test)
predictions = exp(predictions) - 1
write.csv(predictions, file = 'assn1-jmcclellan.txt', row.names = FALSE)
library(gam)
gam.mcount_model = gam(mean_count ~ s(daylabel), data = new_train)
new_train$resids = resid(gam.mcount_model)
gam.resid_model = gam(resids ~ + s(atemp) + s(humidity) + s(windspeed) + weather + hour + season + workingday + holiday, data = new_train)
predictions = predict(gam.mcount_model, test) + predict(gam.resid_model, test)
predictions = exp(predictions) - 1
write.csv(predictions, file = 'assn1-jmcclellan.txt', row.names = FALSE)
library(gam)
gam.mcount_model = gam(mean_count ~ s(daylabel), data = new_train)
new_train$resids = resid(gam.mcount_model)
gam.resid_model = gam(resids ~ + s(atemp) + s(humidity) + s(windspeed) + weather + hour + season + workingday + holiday, data = new_train)
predictions = predict(gam.model, test)
predictions = exp(predictions) - 1
write.csv(predictions, file = 'assn1-jmcclellan.txt', row.names = FALSE)
